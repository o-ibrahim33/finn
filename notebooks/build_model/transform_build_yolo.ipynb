{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Brevitas networks into FINN with the QONNX interchange format\n",
    "\n",
    "**Note: Previously it was possible to directly export the FINN-ONNX interchange format from Brevitas to pass to the FINN compiler. This support is deprecated and FINN uses the export to the QONNX format as a front end, internally FINN uses still the FINN-ONNX format.**\n",
    "\n",
    "In this notebook we'll go through an example of how to import a Brevitas-trained QNN into FINN. The steps will be as follows:\n",
    "\n",
    "1. Load up the trained PyTorch model\n",
    "2. Call Brevitas QONNX export and visualize with Netron\n",
    "3. Import into FINN and converting QONNX to FINN-ONNX\n",
    "\n",
    "We'll use the following utility functions to print the source code for function calls (`showSrc()`) and to visualize a network using netron (`showInNetron()`) in the Jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import onnx\n",
    "from finn.util.visualization import showSrc, showInNetron\n",
    "\n",
    "model_name = \"lpyoloW4A4\"\n",
    "models_dir = os.environ['FINN_ROOT'] + \"/notebooks/\"\n",
    "model_path = models_dir + \"{}_quant.onnx\".format(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize with Netron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine what the exported ONNX model looks like. For this, we will use the Netron visualizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/home/omar/finn/notebooks/lpyoloW4A4_quant.onnx' at http://0.0.0.0:8078\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8078/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f110fb1b070>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running this notebook in the FINN Docker container, you should be able to see an interactive visualization of the imported network above, and click on individual nodes to inspect their parameters. If you look at any of the MatMul nodes, you should be able to see that the weights are all {-1, +1} values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Import into FINN and call cleanup transformations\n",
    "\n",
    "We will now import this ONNX model into FINN using the ModelWrapper, and examine some of the graph attributes from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqonnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcleanup\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cleanup\n\u001b[1;32m      3\u001b[0m model_path_clean \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/omaribrahim/Omar/thesis/finn/notebooks/yolov5s_quant_clean.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m cleanup(\u001b[43mmodel_path\u001b[49m, out_file\u001b[38;5;241m=\u001b[39mmodel_path_clean)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_path' is not defined"
     ]
    }
   ],
   "source": [
    "from qonnx.util.cleanup import cleanup\n",
    "\n",
    "model_path_clean = \"/home/omaribrahim/Omar/thesis/finn/notebooks/yolov1_quant_clean.onnx\"\n",
    "cleanup(model_path, out_file=model_path_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8082\n",
      "Serving '/home/omaribrahim/Omar/thesis/finn/notebooks/yolov5s_quant_clean.onnx' at http://0.0.0.0:8082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8082/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f1ba4eaed10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(model_path_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now import this QONNX model into FINN using the ModelWrapper. Here we can immediatley execute the model to verify correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `QONNXtoFINN` transformation we can convert the model to the FINN internal FINN-ONNX representation. Notably all Quant and BipolarQuant nodes will have disappeared and are converted into MultiThreshold nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And once again we can execute the model with the FINN/QONNX execution engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have succesfully verified that the transformed and cleaned-up FINN graph still produces the same output, and can now use this model for further processing in FINN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Further cleanup and Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (355756226.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(\"input: \",i,i_shape)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print information about input and output tensors\n",
    "for n in model.graph.node:\n",
    "    for i in n.input:\n",
    "        i_shape = model.get_tensor_shape(i)\n",
    "            print(\"input: \",i,i_shape)\n",
    "    for o in n.output:\n",
    "        o_shape = model.get_tensor_shape(o)\n",
    "            print(\"output: \",o,o_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.graph.node[0].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from distutils.dir_util import copy_tree\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "from qonnx.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import (\n",
    "    ApplyConfig,\n",
    "    GiveReadableTensorNames,\n",
    "    GiveUniqueNodeNames,\n",
    "    GiveUniqueParameterTensors,\n",
    "    RemoveStaticGraphInputs,\n",
    "    RemoveUnusedTensors,\n",
    "    ConvertSubToAdd,\n",
    "    ConvertDivToMul,\n",
    "    SortGraph\n",
    ")\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "from qonnx.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from qonnx.util.cleanup import cleanup_model\n",
    "from qonnx.util.config import extract_model_config_to_json\n",
    "from shutil import copy\n",
    "\n",
    "from qonnx.transformation.double_to_single_float import DoubleToSingleFloat\n",
    "from qonnx.transformation.remove import RemoveIdentityOps\n",
    "from finn.transformation.streamline.sign_to_thres import ConvertSignToThres\n",
    "from qonnx.transformation.batchnorm_to_affine import BatchNormToAffine\n",
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "# just for not linear\n",
    "from finn.transformation.streamline.reorder import (\n",
    "    MoveLinearPastEltwiseAdd,\n",
    "    MoveLinearPastFork,\n",
    ")\n",
    "\n",
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.round_thresholds import RoundAndClipThresholds\n",
    "from finn.transformation.streamline.collapse_repeated import (\n",
    "    CollapseRepeatedAdd,\n",
    "    CollapseRepeatedMul,\n",
    ")\n",
    "\n",
    "from finn.transformation.streamline.reorder import (\n",
    "    MoveAddPastMul,\n",
    "    MoveScalarMulPastMatMul,\n",
    "    MoveScalarAddPastMatMul,\n",
    "    MoveAddPastConv,\n",
    "    MoveScalarMulPastConv,\n",
    "    MoveScalarLinearPastInvariants,\n",
    "    MoveMaxPoolPastMultiThreshold,\n",
    "    MoveMulPastMaxPool,\n",
    "    MoveTransposePastFork,\n",
    "    MoveTransposePastScalarMul\n",
    ")\n",
    "\n",
    "from finn.analysis.fpgadataflow.dataflow_performance import dataflow_performance\n",
    "from finn.analysis.fpgadataflow.exp_cycles_per_layer import exp_cycles_per_layer\n",
    "from finn.analysis.fpgadataflow.hls_synth_res_estimation import hls_synth_res_estimation\n",
    "from finn.analysis.fpgadataflow.op_and_param_counts import (\n",
    "    aggregate_dict_keys,\n",
    "    op_and_param_counts,\n",
    ")\n",
    "from finn.analysis.fpgadataflow.res_estimation import (\n",
    "    res_estimation,\n",
    "    res_estimation_complete,\n",
    ")\n",
    "from finn.builder.build_dataflow_config import (\n",
    "    DataflowBuildConfig,\n",
    "    DataflowOutputType,\n",
    "    ShellFlowType,\n",
    "    VerificationStepType,\n",
    ")\n",
    "from finn.core.onnx_exec import execute_onnx\n",
    "from finn.core.rtlsim_exec import rtlsim_exec\n",
    "from finn.core.throughput_test import throughput_test_rtlsim\n",
    "from finn.transformation.fpgadataflow.annotate_cycles import AnnotateCycles\n",
    "from finn.transformation.fpgadataflow.compile_cppsim import CompileCppSim\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.create_stitched_ip import CreateStitchedIP\n",
    "from finn.transformation.fpgadataflow.derive_characteristic import (\n",
    "    DeriveCharacteristic,\n",
    "    DeriveFIFOSizes,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.hlssynth_ip import HLSSynthIP\n",
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "from finn.transformation.fpgadataflow.minimize_accumulator_width import (\n",
    "    MinimizeAccumulatorWidth,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.minimize_weight_bit_width import (\n",
    "    MinimizeWeightBitWidth,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.prepare_cppsim import PrepareCppSim\n",
    "from finn.transformation.fpgadataflow.prepare_ip import PrepareIP\n",
    "from finn.transformation.fpgadataflow.prepare_rtlsim import PrepareRTLSim\n",
    "from finn.transformation.fpgadataflow.replace_verilog_relpaths import (\n",
    "    ReplaceVerilogRelPaths,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.set_exec_mode import SetExecMode\n",
    "from finn.transformation.fpgadataflow.set_fifo_depths import (\n",
    "    InsertAndSetFIFODepths,\n",
    "    RemoveShallowFIFOs,\n",
    "    SplitLargeFIFOs,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.set_folding import SetFolding\n",
    "from finn.transformation.fpgadataflow.synth_ooc import SynthOutOfContext\n",
    "from finn.transformation.fpgadataflow.vitis_build import VitisBuild\n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "from finn.transformation.qonnx.quant_act_to_multithreshold import (\n",
    "    default_filter_function_generator,\n",
    ")\n",
    "from finn.transformation.streamline import Streamline\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC\n",
    "from finn.util.basic import (\n",
    "    get_rtlsim_trace_depth,\n",
    "    pyverilate_get_liveness_threshold_cycles,\n",
    ")\n",
    "from finn.util.pyverilator import verilator_fifosim\n",
    "from finn.util.test import execute_parent\n",
    "from finn.util.visualization import showSrc, showInNetron\n",
    "import onnx\n",
    "from qonnx.util.cleanup import cleanup_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_yolo_tidy(model: ModelWrapper, cfg: DataflowBuildConfig):\n",
    "    model = cleanup_model(model)\n",
    "    model = model.transform(ConvertQONNXtoFINN())\n",
    "    model = model.transform(GiveUniqueParameterTensors())\n",
    "    model = model.transform(InferShapes())\n",
    "    model = model.transform(FoldConstants())\n",
    "    model = model.transform(RemoveStaticGraphInputs())\n",
    "    model = model.transform(GiveUniqueNodeNames())\n",
    "    model = model.transform(GiveReadableTensorNames())\n",
    "    model = model.transform(InferDataTypes())\n",
    "    model = model.transform(InsertTopK())\n",
    "    model = model.transform(InferShapes())\n",
    "    model = model.transform(GiveUniqueNodeNames())\n",
    "    model = model.transform(GiveReadableTensorNames())\n",
    "    model = model.transform(InferDataTypes())\n",
    "    return model\n",
    "\n",
    "def step_yolo_streamline_linear(model: ModelWrapper, cfg: DataflowBuildConfig):\n",
    "    streamline_transformations = [\n",
    "        absorb.AbsorbScalarMulAddIntoTopK(),  # before MoveAddPastMul to avoid int->float\n",
    "        absorb.AbsorbSignBiasIntoMultiThreshold(),\n",
    "        ConvertSubToAdd(),\n",
    "        ConvertDivToMul(),\n",
    "        RemoveIdentityOps(),\n",
    "        CollapseRepeatedMul(),\n",
    "        BatchNormToAffine(),\n",
    "        ConvertSignToThres(),\n",
    "        MoveAddPastMul(),\n",
    "        MoveScalarAddPastMatMul(),\n",
    "        MoveAddPastConv(),\n",
    "        MoveScalarMulPastMatMul(),\n",
    "        MoveScalarMulPastConv(),\n",
    "        MoveScalarLinearPastInvariants(),\n",
    "        MoveAddPastMul(),\n",
    "        CollapseRepeatedAdd(),\n",
    "        CollapseRepeatedMul(),\n",
    "        absorb.AbsorbAddIntoMultiThreshold(),\n",
    "        absorb.FactorOutMulSignMagnitude(),\n",
    "        MoveMulPastMaxPool(),\n",
    "        MoveMaxPoolPastMultiThreshold(),\n",
    "        absorb.AbsorbMulIntoMultiThreshold(),\n",
    "        absorb.Absorb1BitMulIntoMatMul(),\n",
    "        absorb.Absorb1BitMulIntoConv(),\n",
    "        MakeMaxPoolNHWC(),\n",
    "        absorb.AbsorbConsecutiveTransposes(),\n",
    "        absorb.AbsorbTransposeIntoMultiThreshold(),\n",
    "        RoundAndClipThresholds(),\n",
    "    ]\n",
    "    for trn in streamline_transformations:\n",
    "        model = model.transform(trn)\n",
    "        model = model.transform(GiveUniqueNodeNames())\n",
    "    return model\n",
    "\n",
    "def step_yolo_streamline_nonlinear(model: ModelWrapper, cfg: DataflowBuildConfig):\n",
    "    streamline_transformations = [\n",
    "        MoveLinearPastEltwiseAdd(),\n",
    "        MoveLinearPastFork(),\n",
    "    ]\n",
    "    for trn in streamline_transformations:\n",
    "        model = model.transform(trn)\n",
    "        model = model.transform(GiveUniqueNodeNames())\n",
    "    return model\n",
    "\n",
    "def step_yolo_streamline(model: ModelWrapper, cfg: DataflowBuildConfig):\n",
    "\n",
    "    for iter_id in range(8):\n",
    "        model = step_yolo_streamline_linear(model, cfg)\n",
    "        model = step_yolo_streamline_nonlinear(model, cfg)\n",
    "\n",
    "        # big loop tidy up\n",
    "        model = model.transform(RemoveUnusedTensors())\n",
    "        model = model.transform(GiveReadableTensorNames())\n",
    "        model = model.transform(InferDataTypes())\n",
    "        model = model.transform(SortGraph())\n",
    "\n",
    "    model = model.transform(DoubleToSingleFloat())\n",
    "\n",
    "    return model\n",
    "\n",
    "def step_yolo_convert_to_hls(model: ModelWrapper, cfg: DataflowBuildConfig):\n",
    "    model.set_tensor_datatype(model.graph.input[0].name, DataType[\"UINT8\"])\n",
    "    model = model.transform(InferDataLayouts())\n",
    "    model = model.transform(DoubleToSingleFloat())\n",
    "    model = model.transform(InferDataTypes())\n",
    "    model = model.transform(SortGraph())\n",
    "\n",
    "    to_hls_transformations = [\n",
    "        #MinimizeWeightBitWidth,\n",
    "        MinimizeAccumulatorWidth,\n",
    "        to_hls.InferAddStreamsLayer,\n",
    "        LowerConvsToMatMul,\n",
    "        to_hls.InferChannelwiseLinearLayer,\n",
    "        absorb.AbsorbTransposeIntoMultiThreshold,\n",
    "        RoundAndClipThresholds,\n",
    "        to_hls.InferThresholdingLayer,\n",
    "        to_hls.InferQuantizedMatrixVectorActivation,\n",
    "        #MoveTransposePastFork,\n",
    "        MakeMaxPoolNHWC,\n",
    "        absorb.AbsorbConsecutiveTransposes,\n",
    "        to_hls.InferStreamingMaxPool,\n",
    "        to_hls.InferConvInpGen,\n",
    "        to_hls.InferDuplicateStreamsLayer,\n",
    "        to_hls.InferLabelSelectLayer,\n",
    "    ]\n",
    "    for iter_id in range(4):\n",
    "        for trn in to_hls_transformations:\n",
    "            if trn.__name__==\"InferConvInpGen\":\n",
    "                model = model.transform(trn(cfg.force_rtl_conv_inp_gen))\n",
    "            else:\n",
    "                model = model.transform(trn())\n",
    "    \n",
    "            model = model.transform(InferDataLayouts())\n",
    "            model = model.transform(GiveUniqueNodeNames())\n",
    "            model = model.transform(InferDataTypes())\n",
    "        \n",
    "    model = model.transform(RemoveCNVtoFCFlatten())\n",
    "    model = model.transform(GiveReadableTensorNames())\n",
    "    model = model.transform(RemoveUnusedTensors())\n",
    "    model = model.transform(SortGraph())\n",
    "\n",
    "    return model\n",
    "\n",
    "def step_create_dataflow_partition(model: ModelWrapper, cfg: DataflowBuildConfig):\n",
    "    \"\"\"Separate consecutive groups of HLSCustomOp nodes into StreamingDataflowPartition\n",
    "    nodes, which point to a separate ONNX file. Dataflow accelerator synthesis\n",
    "    can only be performed on those HLSCustomOp sub-graphs.\"\"\"\n",
    "\n",
    "    parent_model = model.transform(\n",
    "        CreateDataflowPartition(\n",
    "            partition_model_dir=cfg.output_dir + \"/intermediate_models/supported_op_partitions\"\n",
    "        )\n",
    "    )\n",
    "    sdp_nodes = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")\n",
    "    assert len(sdp_nodes) == 1, \"Only a single StreamingDataflowPartition supported.\"\n",
    "    sdp_node = sdp_nodes[0]\n",
    "    sdp_node = getCustomOp(sdp_node)\n",
    "    dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "    if cfg.save_intermediate_models:\n",
    "        parent_model.save(cfg.output_dir + \"/intermediate_models/dataflow_parent.onnx\")\n",
    "    model = ModelWrapper(dataflow_model_filename)\n",
    "    return model\n",
    "\n",
    "def step_hls_codegen(model: ModelWrapper, cfg: DataflowBuildConfig):\n",
    "    \"Generate Vivado HLS code to prepare HLSCustomOp nodes for IP generation.\"\n",
    "\n",
    "    model = model.transform(PrepareIP(cfg._resolve_fpga_part(), cfg._resolve_hls_clk_period()))\n",
    "    return model\n",
    "\n",
    "def convert_add_mul_to_int(model):\n",
    "    for n in model.graph.node:\n",
    "        if n.op_type == \"Add\" or n.op_type == \"Mul\":\n",
    "            model.set_tensor_datatype(n.name, DataType[\"INT8\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.util.cleanup import cleanup\n",
    "model_name = \"lpyoloW4A4\"\n",
    "\n",
    "model_path = \"/home/omar/finn/notebooks/{}_quant.onnx\".format(model_name)\n",
    "\n",
    "model_path_clean = \"/home/omar/finn/notebooks/{}_quant_clean.onnx\".format(model_name)\n",
    "cleanup(model_path, out_file=model_path_clean)\n",
    "\n",
    "model_path_transformed = \"/home/omar/finn/notebooks/{}_quant_transformed.onnx\".format(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "model_dir = os.environ['FINN_ROOT'] + \"/notebooks\"\n",
    "\n",
    "estimates_output_dir = os.environ['FINN_ROOT'] + \"/notebooks/lpyoloW4A4\"\n",
    "\n",
    "#Delete previous run results if exist\n",
    "# if os.path.exists(estimates_output_dir):\n",
    "#     shutil.rmtree(estimates_output_dir)\n",
    "#     print(\"Previous run results deleted!\")\n",
    "\n",
    "\n",
    "cfg_estimates = build.DataflowBuildConfig(\n",
    "    output_dir          = estimates_output_dir,\n",
    "    mvau_wwidth_max     = 80,\n",
    "    target_fps          = 100,\n",
    "    synth_clk_period_ns = 10.0,\n",
    "    fpga_part           = \"xcu280-fsvh2892-2L-e\",\n",
    "    steps               = build_cfg.yolo_build_steps,\n",
    "    save_intermediate_models = True,\n",
    "    force_rtl_conv_inp_gen = True,\n",
    "    verbose = True,\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# model = ModelWrapper(model_path_add_trans)\n",
    "# model = step_yolo_convert_to_hls(model,cfg_estimates)\n",
    "# model.save(model_path_transformed.split(\".onnx\")[0]+\"hls.onnx\")\n",
    "# model = step_create_dataflow_partition(model,cfg_estimates)\n",
    "# model.save(model_path_transformed.split(\".onnx\")[0]+\"partition.onnx\")\n",
    "\n",
    "model = ModelWrapper(model_path_clean)\n",
    "#model.save(model_path_transformed.split(\".onnx\")[0]+\"before.onnx\")\n",
    "model = step_yolo_tidy(model,cfg_estimates)\n",
    "#model.save(model_path_transformed.split(\".onnx\")[0]+\"tidy.onnx\")\n",
    "model = step_yolo_streamline(model,cfg_estimates)\n",
    "#model.save(model_path_transformed.split(\".onnx\")[0]+\"streamline.onnx\")\n",
    "model = step_yolo_convert_to_hls(model,cfg_estimates)\n",
    "model.save(model_path_transformed.split(\".onnx\")[0]+\"hls.onnx\")\n",
    "model = step_create_dataflow_partition(model,cfg_estimates)\n",
    "#model.save(model_path_transformed.split(\".onnx\")[0]+\"partition.onnx\")\n",
    "model = step_hls_codegen(model,cfg_estimates)\n",
    "#model.save(model_path_transformed.split(\".onnx\")[0]+\"hls_codegen.onnx\")\n",
    "#model.save(model_path_transformed)\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/home/omar/finn/notebooks/lpyoloW4A4_nodetect_quant_transformedtidy.onnx' at http://0.0.0.0:8077\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8077/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f3f707e5cf0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(model_path_transformed.split(\".onnx\")[0]+\"tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8078\n",
      "Serving '/home/omar/finn/notebooks/lpyoloW4A4_unnorm_quant_transformedstreamline.onnx' at http://0.0.0.0:8078\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8078/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f01351ab730>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(model_path_transformed.split(\".onnx\")[0]+\"streamline.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8078\n",
      "Serving '/home/omar/finn/notebooks/lpyoloW4A4_quant_transformedhls.onnx' at http://0.0.0.0:8078\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8078/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f10f417c0d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(model_path_transformed.split(\".onnx\")[0]+\"hls.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8078\n",
      "Serving '/home/omar/finn/notebooks/lpyoloW4A4_nodetect_quant_transformedpartition.onnx' at http://0.0.0.0:8078\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8078/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f01351a8d60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(model_path_transformed.split(\".onnx\")[0]+\"partition.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8082\n",
      "Serving '/home/omar/finn/notebooks/yolov1_quant_transformedhls_codegen.onnx' at http://0.0.0.0:8082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8082/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb63cfb0d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(model_path_transformed.split(\".onnx\")[0]+\"hls_codegen.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8077\n",
      "Serving '/home/omar/finn/notebooks/lpyoloW4A4_quant_clean.onnx' at http://0.0.0.0:8077\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8077/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f1a1fd5aa40>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"/home/omar/finn/notebooks/{}_quant_clean.onnx\".format(\"lpyoloW4A4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8083\n",
      "Serving '/home/omaribrahim/Omar/thesis/finn/notebooks/build/intermediate_models/dataflow_parent.onnx' at http://0.0.0.0:8083\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8083/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7faa5fb77b80>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(estimates_output_dir + \"/intermediate_models/dataflow_parent.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transpose_1_out0', 'Mul_0_param0']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.graph.node[45].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['global_in']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.graph.node[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(model.get_tensor_shape(model.graph.output[0].name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(model.graph.node)):\n",
    "    print(model.get_tensor_datatype(model.graph.node[idx].input[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in model.graph.node:\n",
    "    for i in n.input:\n",
    "        i_shape = model.get_tensor_shape(i)\n",
    "        print(\"input: \",i,i_shape)\n",
    "    for o in n.output:\n",
    "        o_shape = model.get_tensor_shape(o)\n",
    "        print(\"output: \",o,o_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnx.numpy_helper as nph\n",
    "import os\n",
    "\n",
    "# img = cv2.imread(os.environ['FINN_ROOT'] + \"/notebooks/zidane.jpg\")\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# img = cv2.resize(img, (384,640))\n",
    "# img = img/255\n",
    "# # img = np.int8(img)\n",
    "# img = np.reshape(img, (-1,3,384,640))\n",
    "# print(img,img.shape)\n",
    "img = np.load(os.environ['FINN_ROOT'] + \"/notebooks/zidane_NHWC.npy\").reshape(-1,3,384,640)\n",
    "img_tensor = nph.from_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.core.onnx_exec as oxe\n",
    "\n",
    "model_name = \"lpyoloW4A4\"\n",
    "model_path = \"/home/omar/finn/notebooks/{}_quant.onnx\".format(model_name)\n",
    "model_path_clean = \"/home/omar/finn/notebooks/{}_quant_clean.onnx\".format(model_name)\n",
    "#cleanup(model_path, out_file=model_path_clean)\n",
    "\n",
    "model = ModelWrapper(model_path_clean)\n",
    "input_dict = {\"global_in\": nph.to_array(img_tensor)}\n",
    "output_dict = oxe.execute_onnx(model, input_dict)\n",
    "output_model = output_dict[\"global_out\"]\n",
    "#print(output_model,output_model.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.load(os.environ['FINN_ROOT'] + \"/notebooks/output_finn/lpyoloW4A4_test/output.npy\")\n",
    "print(output,output.shape,output.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "def box_iou(box1, box2, eps=1e-7):\n",
    "    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        box1 (Tensor[N, 4])\n",
    "        box2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "\n",
    "    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    (a1, a2), (b1, b2) = box1.unsqueeze(1).chunk(2, 2), box2.unsqueeze(0).chunk(2, 2)\n",
    "    inter = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n",
    "\n",
    "    # IoU = inter / (area1 + area2 - inter)\n",
    "    return inter / ((a2 - a1).prod(2) + (b2 - b1).prod(2) - inter + eps)\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "def non_max_suppression(\n",
    "        prediction,\n",
    "        conf_thres=0.30,\n",
    "        iou_thres=0.6,\n",
    "        classes=None,\n",
    "        agnostic=False,\n",
    "        multi_label=False,\n",
    "        labels=(),\n",
    "        max_det=300,\n",
    "        nm=0,  # number of masks\n",
    "):\n",
    "    \"\"\"Non-Maximum Suppression (NMS) on inference results to reject overlapping detections\n",
    "\n",
    "    Returns:\n",
    "         list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
    "    \"\"\"\n",
    "\n",
    "    # Checks\n",
    "    assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
    "    assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
    "    if isinstance(prediction, (list, tuple)):  # YOLOv5 model in validation model, output = (inference_out, loss_out)\n",
    "        prediction = prediction[0]  # select only inference output\n",
    "\n",
    "    bs = prediction.shape[0]  # batch size\n",
    "    nc = prediction.shape[2] - nm - 5  # number of classes\n",
    "    xc = prediction[..., 4] > conf_thres  # candidates\n",
    "\n",
    "    # Settings\n",
    "    # min_wh = 2  # (pixels) minimum box width and height\n",
    "    max_wh = 7680  # (pixels) maximum box width and height\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "    time_limit = 0.5 + 0.05 * bs  # seconds to quit after\n",
    "    redundant = True  # require redundant detections\n",
    "    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "    merge = False  # use merge-NMS\n",
    "\n",
    "    t = time.time()\n",
    "    mi = 5 + nc  # mask start index\n",
    "    output = [torch.zeros((0, 6 + nm))] * bs\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        x = x[xc[xi]]  # confidence\n",
    "\n",
    "        # Cat apriori labels if autolabelling\n",
    "        if labels and len(labels[xi]):\n",
    "            lb = labels[xi]\n",
    "            v = torch.zeros((len(lb), nc + nm + 5))\n",
    "            v[:, :4] = lb[:, 1:5]  # box\n",
    "            v[:, 4] = 1.0  # conf\n",
    "            v[range(len(lb)), lb[:, 0].long() + 5] = 1.0  # cls\n",
    "            x = torch.cat((x, v), 0)\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Compute conf\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "        # Box/Mask\n",
    "        box = xywh2xyxy(x[:, :4])  # center_x, center_y, width, height) to (x1, y1, x2, y2)\n",
    "        mask = x[:, mi:]  # zero columns if no masks\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        if multi_label:\n",
    "            i, j = (x[:, 5:mi] > conf_thres).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[i], x[i, 5 + j, None], j[:, None].float(), mask[i]), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = torch.max(x[:, 5:mi],1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        # Filter by class\n",
    "        if classes is not None:\n",
    "            x = x[(x[:, 5:6] == torch.tensor(classes)).any(1)]\n",
    "\n",
    "        # Apply finite constraint\n",
    "        # if not torch.isfinite(x).all():\n",
    "        #     x = x[torch.isfinite(x).all(1)]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence and remove excess boxes\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        i = i[:max_det]  # limit detections\n",
    "        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
    "            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
    "            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
    "            weights = iou * scores[None]  # box weights\n",
    "            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
    "            if redundant:\n",
    "                i = i[iou.sum(1) > 1]  # require redundancy\n",
    "\n",
    "        output[xi] = x[i]\n",
    "        if (time.time() - t) > time_limit:\n",
    "            LOGGER.warning(f'WARNING ⚠️ NMS time limit {time_limit:.3f}s exceeded')\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[3.8258e+02, 4.1921e+01, 5.9410e+02, 3.6465e+02, 4.6326e-01, 1.4000e+01],\n",
       "         [2.0233e+02, 1.0822e+02, 3.2895e+02, 3.6109e+02, 3.5337e-01, 1.4000e+01]])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_max_suppression(torch.from_numpy(output_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
